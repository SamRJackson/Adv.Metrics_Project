---
title: "R Notebook"
output: html_notebook
---

# Packages.
_uncomment the first line if the chunk doesn't run_
```{r}
#install.packages("leaps")
#install.packages("ggplot2")
#install.packages("dplyr")
#install.packages("stargazer") #To get summary stat on Latex
#install.packags("mgcv") #To run the Gam 
library(stargazer)
library(readxl)
library(dplyr)
library(ggplot2)
library(leaps)
library(mgcv)
```

# Loading in the dataset.
```{r}
load("/Users/mathis/SFR Cloud/AMSE/Master 2 -ETE-/Advanced_econometrics/Project/dataCrime.RData")
```

# Variable of interest selection.
We start by keeping a wide selection of potential explanatory variables. Choice of variables was based on model specifications in the existing literature around crime. 
```{r}
variables_chosen <- c("communityname","state","population","PopDens","householdsize","PctFam2Par","racepctblack","racePctHisp","agePct16t24","perCapInc","pctWSocSec","pctWInvInc","PctPopUnderPov","PctNotHSGrad","PctUnemployed","ViolentCrimesPerPop","nonViolPerPop")
colnames(concat_test)<-gsub(":","",colnames(concat_test))# remove semicolons from names
concat_dropped <- concat_test[variables_chosen]
```

# Cleaning the datatset
```{r}
is.data.frame(concat_dropped)#test dataframe ok
i=c(3:17) #Select variables to turn into numeric
concat_dropped[ , i] <- apply(concat_dropped[ , i], 2,
                    function(x) as.numeric(as.character(x)))#convert relevant variables in numeric
sapply(concat_dropped, class) #check ok
concat_dropped$crimes=concat_dropped$ViolentCrimesPerPop + concat_dropped$nonViolPerPop #create dependent variable
concat_wo_na= na.omit(concat_dropped)#Drop NA statistics on drops
```

# Using subset selection and Bayesian Information Criteria to select variables
```{r}
drops <- c("communityname", "state", "nonViolPerPop","ViolentCrimesPerPop")
subselect_sample <- select(concat_wo_na,-all_of(drops))
subselect_res <- regsubsets(crimes~., data = subselect_sample, nvmax = 9, method = "forward")

summary(subselect_res)
subselect_sums <- summary(subselect_res)
data.frame(
  Adj.R2 = which.max(subselect_sums$adjr2),
  CP = which.min(subselect_sums$cp),
  BIC = which.min(subselect_sums$bic)
)
```

# Defining the final dataset we will use for estimation
```{r}
final_variables= c("communityname","state","PopDens","householdsize","PctFam2Par","racepctblack","racePctHisp","agePct16t24","PctPopUnderPov","PctNotHSGrad","PctUnemployed","crimes")
dataset=concat_wo_na[final_variables]
is.data.frame(dataset)#test dataframe ok
```
# Summary Statistics
```{r}
stargazer(as.data.frame(dataset))
```


# Gam Model 

Here is the raw version of the Gam model
```{r}
attach(dataset)

Gam <- gam((crimes~s(PopDens)+s(householdsize)+s(PctFam2Par)+s(racepctblack)+s(racePctHisp)+s(agePct16t24)+s(PctPopUnderPov)+s(PctNotHSGrad)+s(PctUnemployed))) 

summary(Gam)
```

According to the results of the tests on the variables in this first model, all variables have a non-linear effect. 

Now we can analyze the graphs. 

```{r}
par(mfrow=c(3,3)) #TO have the 3 graphs side by side
plot(Gam, select=1, shade=T, shade.col="lightblue", rug=T, ylim=c(-5000,1000))
plot(Gam, select=2, shade=T, shade.col="lightblue", rug=T, ylim=c(-1000,7000))
plot(Gam, select=3, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,15000))
plot(Gam, select=4, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,2000))
plot(Gam, select=5, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,8000))
plot(Gam, select=6, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,2000))
plot(Gam, select=7, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,2000) )
plot(Gam, select=8, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,2000))
plot(Gam, select=9, shade=T, shade.col="lightblue", rug=T, ylim=c(-5000,500))
```


For the second GAM, I will simply adjust the smoothing of the functions in order to find the right balance between flexibility and interpretation power. For this, I use the restricted maximum likelihood method that selects automatically the smoothing parameter. For some variables, for which the smoothing is too important, I set this parameter manually. I did not change the number of basis used, as they seem to be already well set up

```{r}
Gam2 <- gam((crimes~s(PopDens,sp=0.01)+s(householdsize)+s(PctFam2Par)+s(racepctblack)+s(racePctHisp)+s(agePct16t24)+s(PctPopUnderPov)+s(PctNotHSGrad)+s(PctUnemployed, sp=0.05)),method="REML")
summary(Gam2)

plot(Gam2, select=1, shade=T, shade.col="lightblue", xlim=c(0,20000), ylim=c(-5000,1000))
plot(Gam2, select=2, shade=T, shade.col="lightblue", rug=T, ylim=c(-1000,7000))
plot(Gam2, select=3, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,15000))
plot(Gam2, select=4, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,2000))
plot(Gam2, select=5, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,8000))
plot(Gam2, select=6, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,2000))
plot(Gam2, select=7, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,2000))
plot(Gam2, select=8, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,2000))
plot(Gam2, select=9, shade=T, shade.col="lightblue", rug=T, ylim=c(-5000,500)) 
```

With the help of the tests and the graphs, the following conclusions can be drawn: 
1) If we observe unemployment with a weaker smoothing parameter, the variable seems non-linear overall. However, it still shows some linearity over the vast majority of observations. 
2) The percentage of the population that is hisapnic seems to act linearly. 
3) With reducing the smooth parameter on the population density function, it capture better the quadratic effect for the low value, but it increases strongly the variance for the higer value. So we will keep the smoothing parameter of the restricted maximum likelihood method. 

Also, one could think that the percentage of people with two parents and household size are related. 

We then take into account these three points for a third GAM model
```{r}
Gam3 <- gam((crimes~s(PopDens)+s(racepctblack)+s(PctPopUnderPov)+s(PctNotHSGrad)+s(agePct16t24,sp=1)+PctUnemployed+racePctHisp+s(PctFam2Par,householdsize)), method="REML")
summary(Gam3)

par(mfrow=c(2,3))
plot(Gam3, select=1, shade=T, shade.col="lightblue", xlim=c(0,20000), ylim=c(-5000,1000))
plot(Gam3, select=2, shade=T, shade.col="lightblue", rug=T, ylim=c(-3000,4000))
plot(Gam3, select=3, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,15000))
plot(Gam3, select=4, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,2000))
plot(Gam3, select=5, shade=T, shade.col="lightblue", rug=T, ylim=c(-2000,2000))
plot(Gam3, select=6, shade=T, shade.col="lightblue", rug=T, ylim=c(1,5))
```


```{r}
par(mfrow=c(2,1))
vis.gam(Gam2, view=c("PctNotHSGrad","agePct16t24"), phi=30, theta=60)
vis.gam(Gam3, view=c("PctNotHSGrad","agePct16t24"), phi=30, theta=60)
```

Note that here we manually set a weaker smoothing term than the one imposed by default, which represented the impact of age as perfectly linear, which is not consistent with the test performed for this model.

# Model checking

```{r}
par(mfrow=c(2,2))
gam.check(Gam3)
```

Here, it reports full convergence. R has found a best solution.
Based on these results, the number of default bases set for each variable appears to be appropriate. 
Tests indicates that residuals are not randomly distributed.
Globalement, les résidus semblent suivrent une distribution normale et centrée sur zéro, mais avec de forts écarts pour des valeurs extrêmes.


# Concurvity 
```{r}
concurvity(Gam3, full = T)
concurvity(Gam3, full = F)
```



